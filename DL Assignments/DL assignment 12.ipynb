{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "How does unsqueeze help us to solve certain broadcasting problems?"
      ],
      "metadata": {
        "id": "MOZEAfG9aagv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In broadcasting, it broadcast on the dim=1 or there must be matching dimension with existing tensor, for this purpose we use the unsqueeze method\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uBTUmvExaalX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the downside of having activations with a standard deviation too far away from 1?"
      ],
      "metadata": {
        "id": "JVdQnufJb0pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If the activations with standard deviation too far away from 1 occur it will create the problem of vanishing or exploading gradient on the basis \n",
        "of direction in which it is far away from 1. \n",
        "If its too small than during backpropogation the weigths will be updated very slowly due to small value of gradient caused by activation. \n",
        "If the value is too large than during backpropogation the gradient will be too lager, so weights and biases update excessively and may lead to untable \n",
        "behavior.\n",
        "Another problem is the lack of generalization in the final model. If the value in the activation have standard deviation is far away from standard \n",
        "deviation of the on which the network was trained on the network will perform poorly.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aPekMzyEb00h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do we show the actual contents of the memory used for a tensor?"
      ],
      "metadata": {
        "id": "fTCC3T1jb3PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For this purpose we have the .storage property of the tensor which shows us the tensor content as it is store in the memory.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "B-kkX_w2b3PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added\n",
        "to each row or each column of the matrix?"
      ],
      "metadata": {
        "id": "CkIOLuuob3SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "import numpy as np\n",
        "a = np.array([1,1,1])\n",
        "b = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
        "print(a, b)\n",
        "res_list = list(map(add, b, a))\n",
        "print(res_list)\n",
        "res_list = list(map(add, a, b))\n",
        "print(res_list)\n",
        "# vector is being added row wise to matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo9NUVxFb3SS",
        "outputId": "2bd5755a-cd0e-4db4-d55d-bd2435dd62a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1] [[1 1 1]\n",
            " [2 2 2]\n",
            " [3 3 3]]\n",
            "[array([2, 2, 2]), array([3, 3, 3]), array([4, 4, 4])]\n",
            "[array([2, 2, 2]), array([3, 3, 3]), array([4, 4, 4])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do broadcasting and expand_as result in increased memory use? Why or why not?"
      ],
      "metadata": {
        "id": "994LGbdUb3VC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "No, they do not as expanding a tesor does not allocate new memory, it only creates a new view on the existing tensor.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "M2Q-7i8Ib3VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement matmul using Einstein summation."
      ],
      "metadata": {
        "id": "6JslCRhub3YS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def matmul(A, B):\n",
        "  return np.einsum('ij, jk -> ik', A, B)\n",
        "\n",
        "A = np.array([[1,2], [3,4]])\n",
        "B = np.array([[5,6], [7,8]])\n",
        "\n",
        "result = matmul(A, B)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "IgIq2ygxb3YT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced791fa-d4e5-45f6-a88b-1083a9cef530"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[19 22]\n",
            " [43 50]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does a repeated index letter represent on the lefthand side of einsum?"
      ],
      "metadata": {
        "id": "TkNtFLVab3bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It represents the summation over that index.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gOAPTWzwb3bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the three rules of Einstein summation notation? Why?"
      ],
      "metadata": {
        "id": "je1xQdWkb3eW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Each index can appear at most twice in any term.\n",
        "Repeated indices are implicitly summed over.\n",
        "Each term must contain identical non-repeated indices.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "c_2Bg2_Ab3eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the forward pass and backward pass of a neural network?"
      ],
      "metadata": {
        "id": "-19DiIyjb3ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "An epoch is composed of a forward and a backward pass, in forward pass we start from the input layer all the way to the top that is the output layer, and \n",
        "in forward pass only the processing takes place no values are update of changed. In backwards pass we go from top layer(output) to (input layer) while \n",
        "updating the weights and bias trying to mitigate the error\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8F8-6bGab3hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do we need to store some of the activations calculated for intermediate layers in the\n",
        "forward pass?"
      ],
      "metadata": {
        "id": "GMahIeY9b3kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The purpose of it is that these activation of the intermidiate layers are used as inputs to the next layer during the forward pass and in order to \n",
        "calculate the activation of the next layer. They also contribute in the calculation of the gradients with respect to the weights and biases of the \n",
        "intermidiate layers. So storing them help to access them directly during the backward pass instead of recalculating these values.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NK6h_Rv4b3kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the downside of having activations with a standard deviation too far away from 1?"
      ],
      "metadata": {
        "id": "TRXh_l87b3oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If the activations with standard deviation too far away from 1 occur it will create the problem of vanishing or exploading gradient on the basis \n",
        "of direction in which it is far away from 1. \n",
        "If its too small than during backpropogation the weigths will be updated very slowly due to small value of gradient caused by activation. \n",
        "If the value is too large than during backpropogation the gradient will be too lager, so weights and biases update excessively and may lead to untable \n",
        "behavior.\n",
        "Another problem is the lack of generalization in the final model. If the value in the activation have standard deviation is far away from standard \n",
        "deviation of the on which the network was trained on the network will perform poorly.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "51_voRgob3oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can weight initialization help avoid this problem?"
      ],
      "metadata": {
        "id": "QeOZyXVfb3rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "One of the helps it does is that if we initialize the weights using a distributaion which has a standard deviation to 1. This solves the problems \n",
        "of vanishing gradient which occurs if the activation with a standard deviation too far away from 1 are present.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IS53JPbAb3rs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "62bca41a-0484-4db2-9add-5c92f48b12ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nOne of the helps it does is that if we initialize the weights using a distributaion which has a standard deviation to 1. This solves the problems \\nof vanishing gradient which occurs if the activation with a standard deviation too far away from 1 are present.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b7nY9ytjqUsm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}