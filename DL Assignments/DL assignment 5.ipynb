{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Why would you want to use the Data API?"
      ],
      "metadata": {
        "id": "L9TS9psx7pWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For building complex input pipelines from simple reusable pieces of code we need the tf.data API. It also contains tf.data.Dataset abstration that represents sequence of elements,\n",
        "in which each element consists of one or more components.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cqoMHPRX7pc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the benefits of splitting a large dataset into multiple files?"
      ],
      "metadata": {
        "id": "8UyP_qt67qGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Splitting a large dataset into multiple files can help making the data independent of each other like we explicitly make training, test, and validation set. So more or \n",
        "less splitting beforehand helps save time. Another would we to fit the data into the memeory One large file would be a problem but several different files when required imported \n",
        "might help, also searching becomes easy as parsing through 1 large dataset is hard where as if we have a oragnised collection of multiple files it becomes easy.\n",
        "It also makes the files pre shuffle the files before shuffling it in the preprocessing step. We can also utilize multiple servers to download the files simultaneously for \n",
        "training the model onto different servers.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SHPI8qls7qGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training, how can you tell that your input pipeline is the bottleneck? What can you do\n",
        "to fix it?"
      ],
      "metadata": {
        "id": "3dqSTF9N7qK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "One way is using the tensorboard to visualize profiling data, see the GPU usage if its not fully utilized than our input pipeline is causing bottleneck.\n",
        "\n",
        "There are various ways to fix it:\n",
        "- we can use multiple thread to read and process the data in parallel, ensuring it prefetches a few batches.\n",
        "- we can look into the preprocessing code if there is room for optimization we can fix from there.\n",
        "- we can also perform some preprocessing beforehand and store the data into multiple files so that we can save time and processing power during training.\n",
        "- we can use a better machine with more cpu, ram, and esure the better bandwidth of the GPU.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xAre9h3A7qK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you save any binary data to a TFRecord file, or only serialized protocol buffers?"
      ],
      "metadata": {
        "id": "IvATlCyu7qOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TFRecord store the data into vary simple binary format that just contains a sequence of binary records of varying size. The binary data could be of any format,\n",
        "but usually it contains serialized protocol buffers.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PnBFuYkV7qOx",
        "outputId": "349797ba-d777-4096-ae57-c0ecec70817c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why would you go through the hassle of converting all your data to the Example protobuf\n",
        "format? Why not use your own protobuf definition?"
      ],
      "metadata": {
        "id": "JvsK5H1c7qSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In tensorflow although it supports all binary format but protobuf is usually used as Tensorflow API contains many inbuilt functions in order to parse the \n",
        "data in an efficient and flexible manner. We can also make our own custom protocol buffer and compile it using protoc. and use the tf.io.decome_proto() function \n",
        "to parse the serialized protobufs.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bOfKX1BL7qSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using TFRecords, when would you want to activate compression? Why not do it\n",
        "systematically?"
      ],
      "metadata": {
        "id": "1cjJKpsX7qWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If we have implemented things in such a manner that during training we need to download the data then it is much better if we activate compression so that \n",
        "the compressed file will be small and will save a lot of time while downloading. But it takes processing power and time to compress files therefore if the data is \n",
        "available locally compression is usually turned off.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VYZRlvMy7qWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data can be preprocessed directly when writing the data files, or within the tf.data pipeline,\n",
        "or in preprocessing layers within your model, or using TF Transform. Can you list a few pros\n",
        "and cons of each option?"
      ],
      "metadata": {
        "id": "ELsQ1Y7b7qZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. for preprocessing directly when writing the data files:\n",
        "pros:\n",
        "- already preprocessed data save training time.\n",
        "- preprocessed data means its already smaller than actual dataset.\n",
        "- easy to materialize the preprocessed data.\n",
        "cons:\n",
        "- as we do not have any functions or methods to preprocess the data the different variants will have to be individually preprocessed.\n",
        "- for data augmentation we will have to materialize many different variants of our dataset wasting space.\n",
        "- we need to add the preprocessed data before the training starts.\n",
        "\n",
        "2. tf.data pipeline:\n",
        "pros:\n",
        "- easy to augement and change preprocessing logics.\n",
        "- we can multithread or prefetch to optimise preprocessing.\n",
        "cons:\n",
        "- but it slows down the training process\n",
        "- preprocessed once per epoch not once per project.\n",
        "\n",
        "3. preprocessing layer withing our model:\n",
        "pros:\n",
        "- helps to write one code for both training and prediction.\n",
        "- as in case we might need to deploy the model to many different platforms, we will not need to write preprocessing code many times.\n",
        "- also helps reducing chances of employing a wrong logic as it is a part of model.\n",
        "cons:\n",
        "- but it slows down training\n",
        "- cannot benefit from multiprocessing as the preprocessing takes place on GPU batchwise.\n",
        "fix:\n",
        "we can lif the preprocessing operation using keras preprocessing layer to run them as a part of tf.data.pipeline so multithreading and prefetch becomes possible.\n",
        "\n",
        "4. TF transform:\n",
        "pros:\n",
        "- each instance is preprocssed just once and data is materialized\n",
        "- automatic generation of data so only need to write the code once.\n",
        "cons:\n",
        "- we need to learn how to use the tool.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XFSNWV9U7qZ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}