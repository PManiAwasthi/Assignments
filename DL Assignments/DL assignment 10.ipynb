{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What does a SavedModel contain? How do you inspect its content?"
      ],
      "metadata": {
        "id": "HFpj98lkVKBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It contains a complete TesorFlow program including training parameters and computation. Does not need the original model building code to run, and makes it useful for sharing \n",
        "or deploying in other tensorflow variants.\n",
        "The show commande in the CLI can be used in order to see the signaturedef, inputs and output and contents of the saved model in hierachicall order.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KYgkfYLFVKHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When should you use TF Serving? What are its main features? What are some tools you can\n",
        "use to deploy it?"
      ],
      "metadata": {
        "id": "M0IDo0ZBVK4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It is a flexible, high-performance serving system for machine learning models, designed for production environments. It makes it easy to deploy new experiment while keeping \n",
        "the server architecture and APIs the same. It provides out of box integration for tensorflow models but can also be extended to serve other types of models and data.\n",
        "Docker can be used to reproduce the TF serving environment as docker image, or we can use TF-Serving CLI.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZXWj4OI2VK4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do you deploy a model across multiple TF Serving instances?"
      ],
      "metadata": {
        "id": "3X8yjH_EVK7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In the config file that is being used for the TF Serving we can specify the multiple instances that we want to deploy for multiple model. In case we need same model we can \n",
        "specify the name of the model so that the TF Serving only will make use of that model.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lyFN3Y2OVK7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When should you use the gRPC API rather than the REST API to query a model served by TF\n",
        "Serving?"
      ],
      "metadata": {
        "id": "L5ZDt8kIVK_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If we need bi-directional streaming where client and server send a sequence of message to each other using the read-write steam.\n",
        "if we need to compress the data instead of using a text format.\n",
        "if we have less bandwidth and need send compressed data better than JSON (as gRPC used protocol buffers)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3jEbVYlFVK_v",
        "outputId": "536f6187-c08f-4004-896c-e73b09db2c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIf we need bi-directional streaming where client and server send a sequence of message to each other using the read-write steam.\\nif we need to compress the data instead of using a text format.\\nif we have less bandwidth and need send compressed data better than JSON (as gRPC used protocol buffers)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the different ways TFLite reduces a model’s size to make it run on a mobile or\n",
        "embedded device?"
      ],
      "metadata": {
        "id": "4VR8SPBFVLDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. Quantization- reducing the precision of the numbers used to represent a model's parameters.\n",
        "2. Prunning- removing parameters of the models that have minor affect on the output.\n",
        "3. clustering- Group the weigths of the model togeters for each layer in a model into a predefined number of clusters, then sharing the controid values for the weights belonging \n",
        "to each inidividual cluster hence reduces number of unique weight values in a model and its comlexity.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LwoZesx8VLDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is quantization-aware training, and why would you need it?"
      ],
      "metadata": {
        "id": "7JN7dbrrVLHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Quantization aware trianing is a way by which we produce a DNN model which is able to run on low end devices such as android by changing its precision from float to \n",
        "lower precision int8 while minimizing loss as much as possible. Here in quant aware training we simulate low precision behaviour in forward pass while the backward pass \n",
        "remains the same. This loss is then minised as it is included in the total loss this leads to a model which is almost lossless.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "IZqRseeDVLHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are model parallelism and data parallelism? Why is the latter\n",
        "generally recommended?"
      ],
      "metadata": {
        "id": "F84CN4fVVLKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data parallelism is when we use the same model for every thread, but feed it with different parts of the data; model parallelism is when you use the same data for every thread, \n",
        "but split the model among threads.\n",
        "Data parallelism might be generally recommended because it is 100% parallel computing and learning uses same model and averages the gradients to get the final gradient for \n",
        "backpropogation so it is much more reliable in terms of obtaining better and complex relationships among the various features of dataset.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tKNnjoyIVLKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training a model across multiple servers, what distribution strategies can you use?\n",
        "How do you choose which one to use?"
      ],
      "metadata": {
        "id": "2fOA8AmyVLOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data-parallel training with synchronous updates (used if device is stronger and with a powerful connection)\n",
        "Data-parallel training with asynchronous updates (if we need can't afford downtime or we have difference in capacity or priorities)\n",
        "Model-parallel training. (used if the model does not fit the GPU)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aCK3ljZvVLOU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}