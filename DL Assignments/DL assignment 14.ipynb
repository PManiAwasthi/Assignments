{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
      ],
      "metadata": {
        "id": "NkT1t1Sf7P5f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TKBEfnl7NkH"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "No. All weights should be initialized to different random values and should not have the same initial value. If weights are symmetrical, meaning they have the same value, \n",
        "it makes it almost impossible for backpropagation to converge to a good solution.\n",
        "\n",
        "Think of it this way: if all the weights are the same, it's like having just one neuron per layer, but much slower.\n",
        "\n",
        "The technique we use to break this symmetry is to sample weights randomly.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is it okay to initialize the bias term to 0?"
      ],
      "metadata": {
        "id": "05bOoYYq7Saw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etc-Ibut7Sax"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Yes, this is fine. It does not end up making much of a difference, it's just that we need to have some kind of provision in measure to deal with occurance of vanishing \n",
        "gradient.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name three advantages of the ELU activation function over ReLU."
      ],
      "metadata": {
        "id": "I0uh7DPe7SeU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6RWAsYx7SeV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "It can take on negative values, so the average output of the neurons in any given layer is typically closer to 0 then when using the ReLU function. This helps alleviate the \n",
        "vanishing gradients problem. The vanishing gradients problem is the idea that gradients often get smaller and smaller as the algorithm progresses down to the lower layers. \n",
        "As a result, the gradient descent update leaves the lower layer connection weights virtually unshanged, and training never converges to a good solution.\n",
        "\n",
        "It always has a non-zero derivative, which avoids the dying units issue that can affect ReLU units. \"Dying ReLU's\" is the problem where units stop outputting anything other \n",
        "than 0. In some cases, you may find that half of your networks neurons are dead, especially if you use a large learning rate.\n",
        "\n",
        "It is smooth everywhere, which helps gradient descent converge faster. ReLU's slope abruptly jumps from 0 to 1 at z = 0. Such an abrupt change can slow down gradient descent \n",
        "because it will bounce around z = 0.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In which cases would you want to use each of the following activation functions: ELU, leaky ReLU, ReLU, tanh, logistic, and softmax?"
      ],
      "metadata": {
        "id": "COeWLblh7SiJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kDX_x3d7SiL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The ELU function is a good default.\n",
        "\n",
        "If you need the network to be as fast as possible, you can use one of the leaky ReLU variants.\n",
        "\n",
        "The basic ReLU function is also preferred due to its simplicity, despite the fact they are generally outperformed by ELU and leaky ReLU. If you have a situation where it's \n",
        "preferable to have neurons output exactly 0, then ReLU is a good choice.\n",
        "\n",
        "tanh can be useful in the output layer if you need your outputs to be between -1 and 1. It's not used too much in hidden layers.\n",
        "\n",
        "The logistic function is useful in the output layer when you need to estimate a probability like in the binary, multi-class, or multi-class multi-label classification problems. \n",
        "Like tanh, it is not used in the hidden layers.\n",
        "\n",
        "The softmax function is useful in the output layer when you need to output probabilities for mutually exclusive classes. Again, it's not used in hidden layers.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What may happen if you set the momentum hyperparameter too close to 1 (0.999999999999) when using the MomentumOptimizer?"
      ],
      "metadata": {
        "id": "9K3DVq_w7SmV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqJyjLlI7SmX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The purpose of the momentum hyperparameter momentum is to simulate friction and prevent the momentum from growing too large. We set it to 0 for high friction, and 1 for no friction.\n",
        "\n",
        "As the hyperparameter gets closer to 1, there will be less friction and the momentum optimization will \"roll faster down the hill.\" This means that the optimizer will overshoot, \n",
        "then come back, overshoot again, and oscillate like this many times before stabilizing at the minimum. This is one of the reasons why it is good to have a bit of friction in the \n",
        "system: it gets rid of these oscillations and thus speeds up convergence.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name three ways you can produce a sparse model?"
      ],
      "metadata": {
        "id": "_G39F-_37Sp2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ-RGK2t7Sp3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "A sparse model is that where most weights are equal to 0. There's a couple ways of achieving that effect.\n",
        "\n",
        "You can train the model normally then zero out tiny weights.\n",
        "\n",
        "For more sparsity, you can apply l1 regularization during training, which pushes the optimizer towards sparsity.\n",
        "\n",
        "Finally, you can combine l1 regulatization with dual averaging using TensorFlow's FTRLOptimizer class.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does dropout slow down training? Does it slow down making predictions on new instances (inference)?"
      ],
      "metadata": {
        "id": "EgLAO4W27Stc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVhKILg77Ste"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Dropout is a popular regularization technique for deep neural networks.\n",
        "\n",
        "The algorithm is: at each training step, every neuron (including the input neurons but excluding the output neurons) has a probability p of being temporariliy \"dropped out\", \n",
        "meaning it will be entierly ignored during this training step. However, it may be active during the next step.\n",
        "\n",
        "The hyperparameter p is called the dropout rate, and it is typically set to 0.5.\n",
        "\n",
        "After training the neurons don't get dropped anymore. That's the gist of the algorithm.\n",
        "\n",
        "Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. Remember, because dropout is only tuned during training,\n",
        "it has no impact on inference.\n",
        "\"\"\""
      ]
    }
  ]
}