{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What is Corpora?"
      ],
      "metadata": {
        "id": "5KNoD_eNUniB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Corpus or Corpora is a collection of written or spoken language samples that have been carefully selected and organized. The sources of which can include books, newspapers, \n",
        "websites and conversations. In NLP we need a lot or words to build a rich vocabulary list and its embedding, and corpus provides us just that as it is diverse and large \n",
        "set of language data. It helps the model learn the structure, style, and vocabulary of a particular language.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3lrfpv9wUnnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are Tokens?"
      ],
      "metadata": {
        "id": "fLfFqSCjUpad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "They are the unit of analyis in a text. They can be words, puctuations, or other units of meaning that are identified and separated from one another in the text. \n",
        "Like in the \"cat sat on the mat\" we have cat, sat, on, the, mat as tokens. They are the typical building blocks of NLP tasks such as text classification, language translation, \n",
        "text summarization, etc.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pGnooSoVUpad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are Unigrams, Bigrams, Trigrams?"
      ],
      "metadata": {
        "id": "d69kdmNqUpe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "They are just types of n-grams, which are sequence of n-tokens in a text. A unigram is a single token like a word, text, etc. A bigram is a sequence of 2 tokens in a text.\n",
        "A trigrams is a siquence of 3 token in a text.\n",
        "\"The cat sat on the mat\",\n",
        "unigrams: The, cat, sat, on, the, mat\n",
        "bigrams: The cat, cat sat, sat on, on the, the mat\n",
        "trigrams: The cat sat, cat sat on, sat on the, on the mat\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eFKXVHRVUpe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to generate n-grams from text?"
      ],
      "metadata": {
        "id": "XBXcvkAkUpjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "First we need to tokenize the data we have then we need the value of n to generate the n-grams, after which we extract the n grams.\n",
        "The extraction of n grams can be done by iterating over the list of tokens obtainedd and grouping them in the sequence of n tokens.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EJEdRH34UpjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain Lemetization?"
      ],
      "metadata": {
        "id": "9f87ZLm4Upns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It is a process of converting words to their base forms, or lemma which is the base form of a word which is often the same as the dictionary form of the word. Like jumping has \n",
        "jump, better has good. Here the approach is to make use of parts-of-speech and definition in a dictionary in order for more accurate but computationaly expensive methods then \n",
        "stemming.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wVtOdEWRUpnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain Stemming?"
      ],
      "metadata": {
        "id": "LVXKv27NUpsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Similar to lemetization as it is also a proecess that aims to reduce the words to their base form, or stem. It is often used to preprocess the data for NLP tasts, so that it can\n",
        "normalize the text and make it easier to analyze. It help in dimensionality reduction, improves the performance of the model. \n",
        "Here we heuristically cut off the end of the words to obtain the root form but while being fast it has a consequence of generating incorrect roots.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QARdudb3Upsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain Part-Of-Speech tagging?"
      ],
      "metadata": {
        "id": "O4Cp3O2wUpwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Is is a process of converting a sentence to forms - list of words, list of tuples where they are associated to a tag which signifies where the word is a noun, adjective, \n",
        "verb, soon.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4Jx6ffG9Upwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain Chunking or shallow processing?"
      ],
      "metadata": {
        "id": "Fsn74R2sUp6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Chunking is a cognitive process involving grouping individual pieces of information into larger, more meaningful units. It helps to simplifiy and organize information, and increases \n",
        "chaces of understanding it. Shallow processing is a term used to describe the process of quickly and superficially processing information. Here we are only focussing on the \n",
        "surface-level details and are not attempting to understand the deeper meanign or context.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "84JbzhidUp6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain Noun Phrase chunking?"
      ],
      "metadata": {
        "id": "UDgUq0QPUp-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Involves identifying and extracting noun phases from a given text. Where a noun pharse is a group of words that functions as a noun in a sectence and include noun or pronoun as \n",
        "the head, alongside other words. It is used to extract and classify the various noun pharse in the text in NLP which helps in information extraction and summarization.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fwedb6HGUp-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain Named Entity Recognition?"
      ],
      "metadata": {
        "id": "KM6Gd49aUqGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Involves identifying and extracting named entites from a given text. They specific people, places, orgarnisation, etc. that are mentioned in the text. It is used to extract and \n",
        "classify named entities in a text.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eLfrW1QbUqGu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}