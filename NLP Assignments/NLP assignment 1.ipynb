{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Explain One-Hot Encoding"
      ],
      "metadata": {
        "id": "CFia8amRH8lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Used to represent the categorical features in a binary vector format, where the length of the vector is equal to the number of possible ouput and we place 1 in the position \n",
        "with respect to the specific category and rest are set to 0, thus obtaining unique binary mapping for each possible category. Use so that the data provided is clear and \n",
        "unambiguous in terms of the categorical variables, but may end up giving a high-dimensional representation if number of categories is large.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3cEFf27nH8qy",
        "outputId": "35603dfc-4a41-48a4-fda3-4d10666b5598"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain Bag of Words"
      ],
      "metadata": {
        "id": "4r_Br7kAH9QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It is a representation of text data that describes the presence of words withing a document, while ignoring the order of words or the context. To make a bag of words \n",
        "first we tokenize the document then makme a vocabulary list then find a numerical representation for that vocabulary list.\n",
        "\n",
        "example::\n",
        "Document 1: \"The cat sat on the mat\"\n",
        "Document 2: \"The dog chased the cat\"\n",
        "\n",
        "The vocabulary for these documents would be [\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"chased\"]. The bag of words representation of Document 1 would be [2, 1, 1, 1, 1, 0, 0], \n",
        "and the bag of words representation of Document 2 would be [2, 1, 0, 0, 0, 1, 1].\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9Fh7grqiH9QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain Bag of N-Grams"
      ],
      "metadata": {
        "id": "_ZLPswDrH9TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It is similar to bag of words but is rather a representation of text data of the document that describes the presence of n-grams(memaning sequences of n consecutive words), \n",
        "and similary does not take into account the context or order of the words.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UeTnyuJvH9TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain TF-IDF"
      ],
      "metadata": {
        "id": "AQW3VItsH9W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Term Frequency - Inverse Document Frequency is a numerical statistic used to reflect the importance of a word in a document or a collection of them. Used in NLP to represent \n",
        "the importance of a word in a document. \n",
        "The TF is frequency of the word in a document, calculated using the number of occurences of the word divided by totall number of words in the document.\n",
        "The IDF is measure of how rare a word is across a collection of documents, calculated using logarithm of the total number of documents divided by the number of documents \n",
        "containing the word.\n",
        "The end result or the TF-IDF score is the product of TF and IDF.\n",
        "If a word is common in a document but rare in the collection its score is high and therefore is considered more important.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HFSSV7U5H9W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is OOV problem?"
      ],
      "metadata": {
        "id": "pNBpm44PH9ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "OOV problem means Out-Of-Vocabulary problem in NLP where a situation arises in which a model encouters a word or phrase that is not present in its vocabulary, or set or \n",
        "word and phrases that it has been trained on. This is a problem as unlike ML models NLP models are typically designed to process and analyze text data by mapping it to a fixed \n",
        "size numberical representation, like word embeddings or one-hot encoding. So due to this if a word is OOV than the NLP model might not be able to represent it numerically and \n",
        "may not be able to process it corectly.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dEmubHcqH9ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are word embeddings?"
      ],
      "metadata": {
        "id": "ZWSIj_PzH9dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Word embeddings are numerical representation of words or phrases that capture the meaning and context of the words in a continuous vector space, commonly used in NLP tasks as \n",
        "a way to represent the meaning of words in a form that can be processed by ML models. The purpose of it is to capture the statistical relationships between the words in the data\n",
        "and map them to a continous vector space. Thus alowing the ML model to represent words in a way that reflect their meaining and context, and capture the similarity between words\n",
        "based on the vectors that represent them. \n",
        "example:\n",
        "Word2Vec\n",
        "GloVe\n",
        "FastText\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XyEKgvxLH9dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain Continuous bag of words (CBOW)"
      ],
      "metadata": {
        "id": "X1Kivs1oH9gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "An unsupervised learning technique used to learn the numerical representation of words or phrased and capture their meaining and context. It is a technique used for learning \n",
        "word embeddings in NLP. It works by predicting a target word given the context words that surround it like in \"the cat sat on\" is used to predict \"mat\".\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vBUQfRVaH9g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain SkipGram"
      ],
      "metadata": {
        "id": "JLVn23z6H9kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The defination is same as above but for working it works by predicting the context words given a target word. Like using \"mat\" it predicts \"the cat sat on\". Here the target \n",
        "word is the input and the context words are output.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "OeEqGgdAH9ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain Glove Embeddings."
      ],
      "metadata": {
        "id": "OmDeLk34H9ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It stands for Global Vector and works by factorizing a matrix of word co-occurrence counts. It uses a global objective function that represents the co-occurrence statistics of \n",
        "the data in low-dimensional space.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_7oSj-CkH9mt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}