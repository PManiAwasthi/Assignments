{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What are sequence to sequence models?"
      ],
      "metadata": {
        "id": "KNZdZ2qlK1jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Sequence to sequence recurrent neural networks (RNNs) are a type of RNN that are used to map an input sequence to an output sequence, typically with the goal of predicting \n",
        "the output sequence given the input sequence. They are commonly used in natural language processing and other tasks that involve analyzing sequential data.\n",
        "\n",
        "Some examples of applications of sequence to sequence RNNs include:\n",
        "\n",
        "- Machine translation: Sequence to sequence RNNs can be used to translate text from one language to another.\n",
        "\n",
        "- Text summarization: Sequence to sequence RNNs can be used to generate a summary of a longer text document.\n",
        "\n",
        "- Speech recognition: Sequence to sequence RNNs can be used to transcribe spoken words into written text.\n",
        "\n",
        "- Dialogue systems: Sequence to sequence RNNs can be used to build chatbots and other systems that can hold natural language conversations with users.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Cs1RQoTCK1qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the problems with Vanilla RNNs?"
      ],
      "metadata": {
        "id": "dyznM-QfK3Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "These are commonly used types of neural networks well suited for tasks involving seqential data, such as NLP and speech recognition. But it can limit the effectiveness of \n",
        "certain tasks. They can struggle with long-term dependencies in the data. As they process the input sequence one time step at a time, updating the hidden state at each time step\n",
        "based on the current input and previous hidden state. But if the size of input sequence gets larger the hidden state may get \"forgotten\" as RNN struggles to capture the overall \n",
        "structure or context of the text. And in case the context depends on context provided by earlier word in the sequence that it create a problem.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lvhIs2MKK3Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is gradient clipping?"
      ],
      "metadata": {
        "id": "VnErInP2K3ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It is technique used to prevent the gradient of NN to become too large or small during the training process. Used often in RNNs to prevent vanishing and exploding gradient, \n",
        "which occur due to the repeated multiplication of gradient over many time steps.\n",
        "So in clipping we clip the gradient ot a max value, above which it can not grow. To implement this the gradients are typically normalized and then multipled by this threshold \n",
        "value if the exceed it.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hhp_8g-mK3ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain Attention mechanism?"
      ],
      "metadata": {
        "id": "RAyCxdixK3eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "A technique in NLP and other tasks involving sequential data to allow the model to \"attend\" to some specific parts of input when processing it. Often used with encoder-decoder \n",
        "models. In each time step the attention-mechanism allows the encode to \"attend\" to some specific parts of the input vercotr and effectively give the model ability to focus \n",
        "on different parts of input when generating the output. It is the weighted sum of the input vectors at each time step, where the weights reflect the importance of each input \n",
        "vector for the current time step. These weights are typically calculated using combination of current hidden state of decoder and input vector and input vecotrs.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2tQ3Qeo_K3eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain Conditional random fields(CRFs)"
      ],
      "metadata": {
        "id": "SHIDvdo3K3jQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "These are type of probabilistic graphical models that are commonly used in the natural language processing and other tasks involving sequential data. These are used to model \n",
        "the dependencies between the ouput variables of a sequence, such as the tags of a sequence of words in natural language sentence, given the input variables such as words.\n",
        "They are based on the idea of statistical model that defines the probability of each possible output sequence given an input sequence. Here each output produces is considered \n",
        "to be dependent on previous step output variabless and the current input variables. The dependencies between the output variables are modeled using a set of transition probabilities,\n",
        "specifying the likelihood of transitioning from one output state to another. And so that are trained using Maximul likelihood estimation.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GKqwDvFnK3jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain self-attention?"
      ],
      "metadata": {
        "id": "66UayjRgK3lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Self attnetion is a technique in deep learning to allow the model to attent to different parts of its own input or output when processing it. Also known as intra-attention, it \n",
        "works by computing a weighted sum of the input or output vector at each time step, wher the weights reflect the importance of each vector for the current time step. These \n",
        "weights are typically a combination of current vector and all the other vecotrs in the sequence and are used to create a weighted sum of the input or output vecotr that is then \n",
        "used as input or ouptut for the model at the current time step.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PkhIErJrK3l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Bahdanau attention?\n"
      ],
      "metadata": {
        "id": "4OisPzwlK3oW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "As we know that in machine translation models the attention mechanism helps the model to focus on specific words or phrases in the source language when generating the output \n",
        "translation. And so it is important because the source and the target languages may have different word order, and the model needs to be able to incorporate with that. \n",
        "In the Bahdanau attention mechanism we calculate the attention weights for each source word, indicating the importance of that word in generating the translation. These weights \n",
        "are calculated using combination of previous hidden state of the decoder and the current hidden state of the encoder.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TxlNVbV4K3oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a language model?"
      ],
      "metadata": {
        "id": "YazYJYdDK3sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "A language model is used to predict the likelihood of a sequence of word. Fundamental to NLP and used in variety of tasks like machine translation, text generation, etc.\n",
        "These models are trained on large corpus of text and learns the statistical patterns and relationships that are present in the words of the language. The aim here is to predict the\n",
        "next word in a sequence, given the word that come before it. They typically evaluated using their perplexity, which is a measure of how well the model is able to predict the \n",
        "next word in a sequence. Low perplexity score indicated the model makes accurate predictions.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "z8EbvXmFK3sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Multi-head attention?"
      ],
      "metadata": {
        "id": "uSVoJ3K3K3v7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We know that in attention mechanism we calculate the attention weights for each input element based on the previous hidden states and the current state, in mult head attention \n",
        "what we do is we calculate multiple such attention weights for each input element, using multiple \"heads\" or attention mechanisms. Where all of them word independently and \n",
        "weights are concatenated and combined to generate the final attention weights. So we can focus on different features simultaneously instead of only one aspect.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ayOHpjkoK3v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Bilingual Evaluation Understudy(BLEU)"
      ],
      "metadata": {
        "id": "mC-mUsCgK3zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It is a metrics use to evaluate the performance of a machine translation system. It was so called because it was originally developed as a substiture for human evaluation of \n",
        "machine translation systems. And is based on the idea that a good machine translation should be similar to a translation produced by a human translator.\n",
        "\n",
        "For the calculation the system first generates a translation of a given source text. Then compare it to one or more reference translations produced by human translators. The \n",
        "Score is calculated based on the number of machine n-grams between the machine translation and the reference translation. The score is then normalized based on the length of \n",
        "the machine translation.\n",
        "Its limitation are that it relies only on matching n-gram but may overlook fluency and coherence factors.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iru_ViKhK3zu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}