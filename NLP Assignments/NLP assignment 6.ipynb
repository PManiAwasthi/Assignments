{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What are Vanilla autoencoders"
      ],
      "metadata": {
        "id": "23HjF5oMTZzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Vanilla autoencoders are the type of NNs that are used for unsupervised learning. The name vanilla is to distinguish them from complext variation of autoencoders that have \n",
        "additional features or modification.\n",
        "We know that an autoencoder has 2 parts an encoder and a decoder, where the encoder maps the input data points to lower dimensional representation (encoding) and the decoder \n",
        "which maps them back to the original input data. The goal is to learn the representation of input data which is efficient, or compact while preserving the important information.\n",
        "The vanilla autoencoder is often used for data compression, dimensionality reduction and feature learning. Applications include anomaly detection, image denoising, etc.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MoGBGOhBTZ8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are sparse autoencoders"
      ],
      "metadata": {
        "id": "g48A6RvvTa7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Sparse autoencoders are a variant of vanilla autoencoders that are used for unsupervised learning. The word \"sparse\" is there because they are designedd to learn the encoding \n",
        "that have relativelly small number of non-zero values or active units compared to total number of units in the encoding.\n",
        "Here we map the input data points to lower dimensional represention in encoder and then the decoder maps them back to original. The sparcity of the encoding is achieved by adding \n",
        "a sparsity penalty term to the objective function that is minimized during training. This penalty encourages the autoencoder that have small number of active units, leading to \n",
        "a more compact and effiecient representation of input data. Useful in hierachical represention of data, where higher level features are learned using lower-level features through \n",
        "successive layers of the autoencoder.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XdbUmzV1Ta7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are denoising autoencoders"
      ],
      "metadata": {
        "id": "3Ly1JqZsTa_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "These are type of autoencoders which are trained to reconstruct a clean version of an input data point from a currupt or noise version of the same data point. \n",
        "It is similar to vanilla autoencoder in term of having encoder and decoder and the working of encoder and decoder is same. But the main goal is to learn a compact representation \n",
        "of the input data that is robust to noise and can be reconstructed from noisy version of original data.\n",
        "In training, it is given a set of clean input data points and it add noise to each data to create a noise version. Then trained to reconstruct the original clean data points from \n",
        "the noise version. Applications include image reconstruction, anomaly detection, etc.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "u90McS6GTa_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "what are convolutional autoencoders"
      ],
      "metadata": {
        "id": "rnrdVfcGTbDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Convolutional autoencoder are the types of autoencoder which use convolutional layers that are used or designed to process data in grid like topology like an image. \n",
        "Similar to vanilla autoencoder has an encoder and decoder here the encoder maps the input data points such as image, to a lower dimensional representation and decoder maps them \n",
        "in reverse. The goal here again is to find efficient representation of input data while preserving important information contained in it. They learn through hierachical \n",
        "representations of data, where high-level features are learned using lower-level features through successing layers of the autoencoder.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "v3CTq-0wTbDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are stacked autoencoders"
      ],
      "metadata": {
        "id": "J1y8569ATbG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "They are made up of autoencoders stacked on top of each other. An autoencoder as we know is used for dimensionality reduction, the stacked autoencoders are trained by feeding them \n",
        "input data and optimizing the weights of the netword so that the output of the network is as close as possible to the input data. Each stacked layer here learns to extract \n",
        "increasingly higher-level features of the data as it passes through the netwek, with the final layer learning to reconstruct the original data from the compressed representation.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "f4G_wMtwTbG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain how to generate sentences using LSTM autoencoders"
      ],
      "metadata": {
        "id": "9HifHGFrTbKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "What we do here is that we train the LSTM network as an autoencoder on a dataset of text. The autoencoder reconstructs its input data, and an LSTM network is good at learning \n",
        "to process sequential data such as text. To generate sentences we first train the network on large data set dividing the text into seqeunces of wrods and trianing the LSTM \n",
        "autoencoder to predict the next word in the sequence given the previous words. Once trained we can use it to generate new sentences by feeding it a seed sequence of words \n",
        "and letting it predict the next word in sequence. The process of geneartion is repeated until desired length of sentence is reached.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3bwzsE7lTbKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain extractive summarization"
      ],
      "metadata": {
        "id": "Lp5bqDxJTbPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It is a type of summarization that involves selecting the most important sentences and phrases from the document and including them in the summary. The goad is to create a summary \n",
        "reflecting the main points and information contained in the original document.\n",
        "For this we typically identify the key points and main ideas of the document and select the sentences and phrases that best represent these ideas, we can use NLP to find nouns \n",
        "phrases or we can use ML algorithms to identify the most representative sentences on their frequency or other statistical measures. We can create news articles, scientific papers, \n",
        "etc.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VQBD-ZNkTbPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "explain abstractive summarization"
      ],
      "metadata": {
        "id": "ix8qttmATbSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Type of summarization involving the generation of summary of a document with new phrases and sentences that capture the main ideas and information of the original document. \n",
        "here a new summary from scratch is created using combination of NLP and ML algorithms. We typically analyze the content of the document and identify key points. We then use it \n",
        "to generate a summary reflecting them. It is quite complex and challenging task than extractive summarization due to its deeper understanding of the content of document and \n",
        "the ability to generatee coherent and concise summaries.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MVUH4ULhTbSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "explain beam search"
      ],
      "metadata": {
        "id": "i-WccjvNTbXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It is a algorithm used to find a optimal solution to a search problem by exploring the limited set of possible soultion at each step, often used in NLP and machine translation tasks.\n",
        "Here the search space is explored by considering a fixed number of the most promising candidates at each step, called the \"beam size\". It is determined by the number of candidates \n",
        "considered at each step, with large beam size resulting in a more thorough search but longer computation time.\n",
        "Here at each step, the algorithm evaluates the candidates and selects the ones that are most liekely to lead to the optimal solution, based on some evaluation criterion, such as \n",
        "likelihood of the candidate sequence given the data. The selected candidates are then expanded by adding the next word or phrase to the seqeunce, and process is repeated unitl the \n",
        "desired length ouput is reached.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SV5tO4RLTbXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "explain length normalization"
      ],
      "metadata": {
        "id": "k0iNnPfYTba4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Length normalization is a technique that is used to adjust the relative importance of different elements in a dataset based on their length. In NLP we have often used this where \n",
        "the length of the input data, such as a sentence or document can vary significantly. Length normalization ensure that longer elements are not given more weights that shorter \n",
        "elements when compared or aggregated. Is is important in text classification, when have longer documents with a lot of info which might not be useful.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "h5112h7uTba5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "explain coverage normalization"
      ],
      "metadata": {
        "id": "pzeZ-ZbOTbek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "It is a technique that is used to adjust the relative important of different elements of the dataset based on their coverage of the data. Often used in NLP tasks to generate \n",
        "summaries. It ensures that the elements that cover a larger portion of the data are not given more weight than the elements that cover a smaller portion. Useful in text summarization \n",
        "as the element may not be representive just because it has larger coverage.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MaodN_igTbel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "explain ROUGE metric evaluation"
      ],
      "metadata": {
        "id": "zayRvSQuTbik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "A ROUGE metric evaluation or (Recall Oriented Understudy for Gisting Evaluation) metric is a commonly used evaluation measure for text summarization tasks. Use to evaluate \n",
        "the quality of summarization and calcualte the overlap between the 2.\n",
        "Several variations include ROUGE-L, ROUGE-N, and ROUGE-W, which measure the overlap between the generated summary and reference summary from different level of granuality.\n",
        "Here N measure using N-grams, L measures using level of longest common subsequence which is longest sequence present in both generated and reference summary, and W measure level \n",
        "of longest common subsequence taking in account importannce of different words in the summary.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CFPu4J0UTbim"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}